{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook overview\n",
    "1. Read in the data (function)\n",
    "    - Making sure that the data is in the correct format\n",
    "    - Function does the label mapping and conversion of labels to label ids\n",
    "\n",
    "2. Transform data into Huggingface Dataset object\n",
    "\n",
    "3. Tokenize and align labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoConfig, RobertaTokenizerFast, DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from evaluate import load \n",
    "from span_f1 import readNlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_train = \"en_ewt-ud-train.iob2\"\n",
    "path_dev = \"en_ewt-ud-dev.iob2\"\n",
    "path_test = \"en_ewt-ud-test-masked.iob2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model name\n",
    "model_name = 'deepset/roberta-base-squad2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the label to id mapping \n",
    "data_labels = readNlu(path_train) # reads in label column\n",
    "\n",
    "label_set = set()\n",
    "\n",
    "for labels in data_labels:\n",
    "    label_set.update(labels)\n",
    "\n",
    "num_labels = len(label_set)\n",
    "\n",
    "label2id = {label: id for id, label in enumerate(label_set)}\n",
    "\n",
    "id2label = {id: label for label, id in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading iob2 data (from solution for assignment 5)\n",
    "def read_iob2_file(path):\n",
    "    '''\n",
    "    This function reads iob2 files\n",
    "    \n",
    "    Parameters:\n",
    "    - path: path to read from\n",
    "\n",
    "    Returns:\n",
    "    - list with dictionaries for each sentence where the keys are 'tokens', 'ner_tags', and 'tag_ids' and \n",
    "      the values are lists that hold the tokens, ner_tags, and tag_ids.\n",
    "    '''\n",
    "\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "    current_tag_ids = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip() # removes any leading and trailing whitespaces from the line\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#': \n",
    "                continue # skip comments\n",
    "\n",
    "            # splitting at 'tab', as the data is tab separated \n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            # add the entry in the second colun (the word) to current_words\n",
    "            current_words.append(tok[1]) \n",
    "\n",
    "            # add the current tag \n",
    "            current_tags.append(tok[2]) \n",
    "\n",
    "            # add the current tag mapped to the corresponding id (int)\n",
    "            current_tag_ids.append(label2id[tok[2]]) \n",
    "        \n",
    "        else: # skip empty lines\n",
    "            if current_words: # if current_words is not empty\n",
    "\n",
    "                # add entry to dict where tokens and ner_tags are keys and the values are lists\n",
    "                data.append({\"tokens\": current_words, \"ner_tags\": current_tags, \"tag_ids\": current_tag_ids})\n",
    "\n",
    "            # start over  \n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "            current_tag_ids = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append({\"tokens\": current_words, \"ner_tags\": current_tags, \"tag_ids\": current_tag_ids})\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "train_data = read_iob2_file(path_train)\n",
    "dev_data = read_iob2_file(path_dev)\n",
    "test_data = read_iob2_file(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to huggingface format\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "dev_dataset = Dataset.from_list(dev_data)\n",
    "test_dataset = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and align labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\n",
    "    model_name, \n",
    "    use_fast = True, \n",
    "    add_prefix_space = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the function \n",
    "def tokenize_and_align_labels(data):\n",
    "    '''\n",
    "    This function tokentizes the tokens and align the labels to the newly created subwords.\n",
    "    The tokens can be split into multiple subwords, which are marked with -100, so they are ignored\n",
    "    in the model *********\n",
    "\n",
    "    Parameters:\n",
    "        - data : the data we wish to tokenize and align. Must be a Huggingface dataset.\n",
    "\n",
    "    Returns: \n",
    "        - the tokenized input with aligned labels.\n",
    "    '''\n",
    "\n",
    "    # tokenize the input\n",
    "    tokenized_inputs = tokenizer(\n",
    "        data[\"tokens\"],             # tokenize the tokens (words)\n",
    "        is_split_into_words = True, # tells the tokenizer each item in the list is already a separate word/token\n",
    "        truncation = True,          # if a sentence is longer than the max_length it will be truncated / cut off \n",
    "        max_length = 128,           # a sentence can't be longer than 128\n",
    "        padding = False             # no padding to save memory\n",
    "    )\n",
    "\n",
    "    \n",
    "    # create empty list for aligned labels (to the subwords)\n",
    "    all_labels = []\n",
    "\n",
    "    # loop through each sentence\n",
    "    for batch_index, labels in enumerate(data[\"tag_ids\"]): \n",
    "        \n",
    "        # 'word_ids()' returns a list the same length as the subword-tokens,\n",
    "        # each entry telling you which 'word' or token it came from\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index = batch_index)  \n",
    "        \n",
    "        label_ids = []\n",
    "        prev_word_id = None  \n",
    "\n",
    "        # loop through the ids of the subword-tokens \n",
    "        for word_id in word_ids:\n",
    "\n",
    "            if word_id is None:\n",
    "                # e.g. special tokens or padding => ignore\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            elif word_id == prev_word_id:\n",
    "                # subword token of the same word => ignore\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            else:\n",
    "                # new subword, so use the label for the original token\n",
    "                label_ids.append(labels[word_id])\n",
    "            \n",
    "            # move on to the next word\n",
    "            prev_word_id = word_id\n",
    "        \n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    # add the new algined labels to the tokenized inputs\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched = True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_dev_dataset = dev_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model and config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels = num_labels, \n",
    "    id2label = id2label, \n",
    "    label2id = label2id\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype = 'auto', \n",
    "    config = config\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are running the code on a Mac so we use MPS. Might need to change it depending on the machine the code is run on (ex. HPC)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"output_trainer\", \n",
    "    eval_strategy = 'epoch', \n",
    "    save_strategy = \"no\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fuction to convert prediction into labels\n",
    "def pred2label(predictions):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    logits, labels = predictions # unpack predictons into logits (probabilities) and labels\n",
    "\n",
    "    preds = np.argmax(logits, axis = -1) # choose the highest probability as the prediciton\n",
    "\n",
    "    true_labels = [] # list to hold true labels\n",
    "    pred_labels = []  # list to hold predicted labels\n",
    "\n",
    "    # convert true labels and predictions to string\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "\n",
    "        true_labels.append([id2label[label] for label in label_seq if label != -100])\n",
    "        \n",
    "        pred_labels.append([id2label[pred] for pred, label in zip(pred_seq, label_seq) if label != -100])\n",
    "\n",
    "    return true_labels, pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"seqeval\")  # load the seqeval metric \n",
    "\n",
    "# define a function for computing metrics during training\n",
    "def compute_metrics(predictions):\n",
    "    '''\n",
    "    This function computes precision, recall, f1 and accuracy.\n",
    "\n",
    "    Parameters: \n",
    "    - predictions\n",
    "    '''\n",
    "    true_labels, pred_labels = pred2label(predictions)\n",
    "\n",
    "    results = metric.compute(predictions = pred_labels, references = true_labels)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": results[\"overall_precision\"],\n",
    "        \"Recall\": results[\"overall_recall\"],\n",
    "        \"F1-score\": results[\"overall_f1\"],\n",
    "        \"Accuracy\": results[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    eval_dataset = tokenized_dev_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"output_trainer\")\n",
    "tokenizer.save_pretrained(\"output_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the dev set\n",
    "- For checking model performance and if formatting is correct with span_f1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "dev_preds, true_labels, _ = trainer.predict(tokenized_dev_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict max logit and convert to strings\n",
    "dev_labels, dev_predictions = pred2label((dev_preds, true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_iob2_file(data, predictions = None, path = None, gold = False):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # formatting the predictions on dev set\n",
    "    format = []\n",
    "\n",
    "    # Loop through all items in dev_data\n",
    "    for i in range(len(predictions)):\n",
    "        if gold:\n",
    "            format.append((data[i]['tokens'], (data[i]['ner_tags'])))\n",
    "        else:\n",
    "            # Access 'tokens' in dev_data[i] and append it with the corresponding prediction\n",
    "            format.append((data[i]['tokens'], predictions[i]))\n",
    "\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as f:\n",
    "        for sentence in format:\n",
    "            words, labels = sentence\n",
    "            for idx, (word, label) in enumerate(zip(words, labels), start = 1):\n",
    "                f.write(f\"{idx}\\t{word}\\t{label}\\t-\\t-\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the iob2 file with the gold labels for dev-set\n",
    "write_iob2_file(dev_data,  path = \"dev_gold.iob2\", gold = True)\n",
    "\n",
    "# writing the iob2 file with the predicted labels for dev-set\n",
    "write_iob2_file(dev_data, predictions = dev_predictions, path = \"dev_output.iob2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds, _, _ = trainer.predict(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict max logit and convert to strings\n",
    "test_predictions = pred2label((test_preds, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write output file for predictions on test data\n",
    "write_iob2_file(test_data, predictions = test_predictions, path = \"test_predictions.iob2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
